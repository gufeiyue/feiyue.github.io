<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[上海链家爬虫2]]></title>
      <url>%2F2017%2F01%2F30%2F%E4%B8%8A%E6%B5%B7%E9%93%BE%E5%AE%B6%E7%88%AC%E8%99%AB2%2F</url>
      <content type="text"><![CDATA[上海链家数据分析上篇我们爬取了上海链家的数据，我们可以使用 tableau 等工具做出房价分布图，下图是我爬取的南京房价做出的图，颜色越深房价越高，点越大房源越多（百度获取的坐标整体有偏移，可算出偏移整体修改）： 我们怎么才能做出类似的图呢？1、我们需要经纬度​ 这个我们可以调用百度地图的 api 去实现，我写了个 python 脚本去处理： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#!/usr/bin/env python#-*- coding:utf-8 -*-import urllib2import timefrom bs4 import BeautifulSoupimport re, requests, jsonimport MySQLdbimport HTMLParserimport sysreload(sys)sys.setdefaultencoding('utf8')def getpage(area): url = "http://api.map.baidu.com/geocoder/v2/?address=%s&amp;city='南京'&amp;output=json&amp;ak=SYRC49V2SpTfNLm52F89eO57MKcRgKAA" % area infos = urllib2.urlopen(url) for info in infos: print info['location']def getinfo(area): url = "http://api.map.baidu.com/geocoder/v2/?address=%s&amp;city='南京'&amp;output=json&amp;ak=SYRC49V2SpTfNLm52F89eO57MKcRgKAA" % area print url r = requests.get(url) s = json.loads(r.text) print s['status'] if s['status'] == 0: info = s['result']['location'] return info else: passdef calculation(): sql = "select DISTINCT `where` from nj_all_1230;" info = db_query(sql) number = num(sql) i = number while i &gt;= 1: where = info[number-i][0] infos = getinfo(where) if infos: lat = infos['lat'] lng = infos['lng'] db_update_lat(lat, lng, where) else: pass i = i - 1###DB###############################################################def connect(): conn= MySQLdb.connect( host='127.0.0.1', port = 3306, user='lianjia', passwd='gufy123', db ='lianjia', use_unicode=True, charset="utf8" ) return conn#按照条件搜索一共多少条数据def num(select_sql): conn=connect() cur = conn.cursor() #获得表中有多少条数据 number = cur.execute(select_sql) return number conn.close()#按照条件搜索所有表中数据def db_query(sql): conn=connect() cur = conn.cursor() cur.execute(sql) info = cur.fetchall() return info conn.close()def db_update_lat(lat, lng, area): update_sql = "update `nj_all_1230` set `lat`='%s',`lng` = '%s' where `where` = '%s';" sql = update_sql % (lat, lng, area) #print sql #print sql conn=connect() cur = conn.cursor() cur.execute(sql) conn.commit() conn.close()##########################################################################if __name__ == '__main__': calculation() 2、使用 tableau​ 我们获得了经纬度，然后扔到 tableau 即可，我们可以分析出不少东西 1、可以根据这些二手房价格数据获得房价高低分布图，看看土豪们喜欢买哪儿 2、根据链家部分房源有距离地铁多远的参数，可以提取比如小于1000米的地铁房，看看哪站地铁房价便宜，上班又方便 3、如果有楼盘对应学区信息，可以分析出该学区对应的哪个楼盘价格最低]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[上海链家数据爬取分析（1）]]></title>
      <url>%2F2017%2F01%2F30%2F%E4%B8%8A%E6%B5%B7%E9%93%BE%E5%AE%B6%E7%88%AC%E8%99%AB%2F</url>
      <content type="text"><![CDATA[一、上海链家数据爬取很久之前写的一个爬取上海链家全部房源的脚本，写的比较 low按照每个大区下面的小区域进行单独的爬取，获取到数据存进数据库，根据小区 url 获取均价，最后处理挂牌价与均价，可以寻找低价楼盘，不过低价一般都是商住两用、顶层、朝向不太好、户型不方正，还需注意。下面给出代码仅供参考：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246#!/usr/bin/env python#-*- coding:utf-8 -*-import urllib2 import time from bs4 import BeautifulSoup import reimport MySQLdbimport HTMLParserdef getpage(area): url = 'http://sh.lianjia.com/ershoufang/'+str(area) page = urllib2.urlopen(url) soup = BeautifulSoup(page) page = soup.find("a", gahref="results_totalpage") try: totalpage = page.get_text() return int(totalpage) except : return 1 def getnew(area, page): for i in range(1,page+1): print "当前区域:", area print "当前区域:", i url = 'http://sh.lianjia.com/ershoufang/'+str(area)+'/d'+str(i) #print url page = urllib2.urlopen(url) soup = BeautifulSoup(page) for link in soup.find_all('div','info-panel'): url = link.a.get('href') url1 = 'http://sh.lianjia.com' + url title = link.a.get_text() #print title #print link.get_text() where_list = link.find_all('div','where') for where in where_list: where1 = where.get_text() xiaoqu_url = where.a.get('href') xiaoqu_url1 = 'http://sh.lianjia.com/' + xiaoqu_url #print where1 #print xiaoqu_url1 other_list = link.find_all('div','other') for other in other_list: other1 = other.get_text() #print other1 agency_list = link.find_all('div','left agency') for agency in agency_list: agency1 = agency.get_text() #print agency1 totalprice_list = link.find_all('div','price') for totalprice in totalprice_list: total_Price1 = totalprice.span.string #print total_Price1 price_list = link.find_all('div','price-pre') for price in price_list: unit_Price1 = price.get_text() #print unit_Price1 db_insert(title, where1, other1, agency1, total_Price1, unit_Price1, url1, xiaoqu_url1, area)def delete_sql1(): sql1 = "UPDATE sh_fengxian SET `where`=REPLACE(`where`,CHAR(10),'');" sql2 = "UPDATE sh_fengxian SET `where`=REPLACE(`where`,CHAR(13),'');" sql3 = "UPDATE sh_fengxian SET `where`=REPLACE(`where`,' ','');" sql4 = "UPDATE sh_fengxian SET `other`=REPLACE(`other`,CHAR(10),'');" sql5 = "UPDATE sh_fengxian SET `other`=REPLACE(`other`,CHAR(13),'');" sql6 = "UPDATE sh_fengxian SET `other`=REPLACE(`other`,' ','');" sql7 = "UPDATE sh_fengxian SET `agency`=REPLACE(`agency`,CHAR(10),'');" sql8 = "UPDATE sh_fengxian SET `agency`=REPLACE(`agency`,CHAR(13),'');" sql9 = "UPDATE sh_fengxian SET `agency`=REPLACE(`agency`,' ','');" delete(sql1) delete(sql2) delete(sql3) delete(sql4) delete(sql5) delete(sql6) delete(sql7) delete(sql8) delete(sql9)###DB###############################################################def calculation(): sql = "select * from sh_fengxian where `proportion` is NULL;" info = db_query(sql) number = num(sql) i = number while i &gt;= 1: unit_price = info[number-i][6] average_price = info[number-i][8] #print sub_password #print unit_price unit_price1 = float(unit_price) average_price1 = float(average_price) proportion = unit_price1/average_price1 print proportion try: db_update_proportion(proportion, unit_price, average_price) except: pass i = i -1def getprice(url): average_price2 = 1 listing_price2 = 1 page = urllib2.urlopen(url) soup = BeautifulSoup(page) for priceInfo in soup.find_all('div','priceInfo'): for listing_price in priceInfo.find_all('div', 'item col1'): for listing_price1 in listing_price.find_all('span', 'p'): try: listing_price2 = listing_price1.get_text() #print listing_price2 except: pass for average_price in priceInfo.find_all('div', 'item'): for average_price1 in average_price.find_all('span', 'p'): try: average_price2 = average_price1.get_text() #print average_price2 except: pass db_update_price(listing_price2, average_price2, url)def price_url(): sql = "SELECT DISTINCT average_url FROM sh_fengxian where `average_price` is null;" info = db_query(sql) number = num(sql) #print number i = number while i &gt;= 1: price_url = info[number-i][0] print price_url getprice(price_url) #print average_price i = i - 1def delete_sql2(): sql1 = "UPDATE sh_fengxian SET `average_price`=REPLACE(`average_price`,CHAR(10),'');" sql2 = "UPDATE sh_fengxian SET `average_price`=REPLACE(`average_price`,CHAR(13),'');" sql3 = "UPDATE sh_fengxian SET `average_price`=REPLACE(`average_price`,' ','');" sql4 = "UPDATE sh_fengxian SET `listing_price`=REPLACE(`listing_price`,CHAR(10),'');" sql5 = "UPDATE sh_fengxian SET `listing_price`=REPLACE(`listing_price`,CHAR(13),'');" sql6 = "UPDATE sh_fengxian SET `listing_price`=REPLACE(`listing_price`,' ','');" sql7 = "UPDATE sh_fengxian SET `unit_Price`=replace(`unit_Price`,'元/平','');" delete(sql1) delete(sql2) delete(sql3) delete(sql4) delete(sql5) delete(sql6) delete(sql7)###DB###############################################################def connect(): conn= MySQLdb.connect( host='127.0.0.1', port = 3306, user='lianjia', passwd='gufy123', db ='lianjia', use_unicode=True, charset="utf8" ) return conn#按照条件搜索一共多少条数据def num(select_sql): conn=connect() cur = conn.cursor() #获得表中有多少条数据 number = cur.execute(select_sql) return number conn.close()#按照条件搜索所有表中数据def db_query(sql): conn=connect() cur = conn.cursor() cur.execute(sql) info = cur.fetchall() return info conn.close()def db_insert(title, where1, other1, agency1, total_Price1, unit_Price1, url1, xiaoqu_url1, area): update_sql = "INSERT INTO `sh_fengxian` (`id`, `title`, `where`, `other`, `agency`, `total_price`, `unit_Price`, `average_price`, `proportion`, `url`, `average_url`, `area`, `remark`) VALUES (NULL, '%s', '%s', '%s', '%s', '%s', '%s', NULL, NULL, '%s', '%s', '%s', '');" sql = update_sql % (title, where1, other1, agency1, total_Price1, unit_Price1, url1, xiaoqu_url1, area) #print sql conn=connect() cur = conn.cursor() cur.execute(sql) conn.commit() conn.close()def db_update_proportion(proportion, unit_price, average_price): update_sql = "update `sh_fengxian` set `proportion`='%s' where `unit_Price` = '%s' and `average_price` = '%s';" sql = update_sql % (proportion, unit_price, average_price) conn=connect() cur = conn.cursor() cur.execute(sql) conn.commit() conn.close()def db_update_price(listing_price, average_price, average_url): update_sql = "update `sh_fengxian` set `listing_price`='%s',`average_price`='%s' where `average_url` = '%s';" sql = update_sql % (listing_price, average_price, average_url) print sql #print sql conn=connect() cur = conn.cursor() cur.execute(sql) conn.commit() conn.close()def delete(sql): conn=connect() cur = conn.cursor() cur.execute(sql) conn.commit() conn.close()##########################################################################if __name__ == '__main__': sh_fengxian = ('fengcheng', 'fengxianjinhui', 'haiwan', 'nanqiao', 'qingcun', 'situan', 'xidu', 'zhuanghang', 'zheli') for area in sh_fengxian: page = getpage(area) getnew(area, page) delete_sql1() price_url() delete_sql2() calculation() 表结构： 1234567891011121314151617CREATE TABLE `sh_fengxian` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `title` varchar(200) DEFAULT NULL, `where` varchar(200) DEFAULT NULL, `other` varchar(200) DEFAULT NULL, `agency` varchar(200) DEFAULT NULL, `total_price` varchar(200) DEFAULT NULL, `unit_Price` varchar(200) DEFAULT NULL, `listing_price` varchar(200) DEFAULT NULL, `average_price` varchar(200) DEFAULT NULL, `proportion` varchar(200) DEFAULT NULL, `url` varchar(200) DEFAULT NULL, `average_url` varchar(200) DEFAULT NULL, `area` varchar(200) DEFAULT NULL, `remark` varchar(200) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3857 DEFAULT CHARSET=utf8;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[jstree 使用]]></title>
      <url>%2F2017%2F01%2F28%2Fflask-jstree%2F</url>
      <content type="text"><![CDATA[jstree、flask 前后端交互展示文件内容1、遍历目录生成 json 文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import os import glob import json def fun(path,parent): global Id global jsonstr global count for i,fn in enumerate(glob.glob(path + os.sep + '*' )): if os.path.isdir(fn): jsonstr+='''&#123;"id":"'''+ str(Id)+'''","text":"'''+os.path.basename(fn)+'''","children":[''' parent=Id Id+=1 for j,li in enumerate(glob.glob(fn + os.sep + '*' )): if os.path.isdir(li): jsonstr+='''&#123;"id":"'''+ str(Id)+'''","text":"'''+os.path.basename(li)+'''","children":[''' parent=Id Id+=1 fun(li,parent) jsonstr+="]&#125;" if j&lt;len(glob.glob(fn + os.sep + '*' ))-1: jsonstr+="," else: jsonstr+='''&#123;"id":"'''+ str(Id)+'''","text":"'''+os.path.basename(li)+'''","icon":"fa fa-file-text-o"&#125;''' Id+=1 if j&lt;len(glob.glob(fn + os.sep + '*' ))-1: jsonstr+="," jsonstr+="]&#125;" if i&lt;len(glob.glob(path + os.sep + '*' ))-1: jsonstr+="," else: jsonstr+='''&#123;"id":"'''+ str(Id)+'''","text":"'''+os.path.basename(fn)+'''","icon":"fa fa-file-text-o"&#125;''' Id+=1 if i&lt;len(glob.glob(path + os.sep + '*' ))-1: jsonstr+="," return jsonstr path="/Users/gufy/Desktop/test/config" parent=0 Id=0 jsonstr="[" jsonstr=fun(path,0) jsonstr+="]" file_object = open('root.json', 'w') file_object.write(jsonstr) file_object.close() 2、后端路由实现12345678910@admin.route('/config_tree', methods=['GET', 'POST'])def config_tree(): path = "/Users/gufy/Desktop/root.json" file_object = open(path) try: infos = file_object.read() finally: file_object.close() print type(infos) return infos 3、前端1&lt;div id="using_json"&gt;&lt;/div&gt; 12345678$('#using_json').jstree(&#123; 'core' : &#123; 'data': &#123; "url" : "/config_tree", "dataType" : "json" &#125; &#125;&#125;) 需要注意的是返回的 json 双引号保持一致，不然 jstree 解析会失败]]></content>
    </entry>

    
  
  
</search>
